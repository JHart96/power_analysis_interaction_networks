---
title: "Estimating Accuracy of Sampling with Interaction Rate Data"
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---

Load in ggplot for visualisations.

```{r setup}
knitr::opts_chunk$set(cache = TRUE)
library(ggplot2)

cp <- c("#003f5c", "#2f4b7c", "#665191", "#a05195", "#d45087", "#f95d6a", "#ff7c43", "#ffa600")
```

Define log-likelihood function of negative binomial with gamma parameters (in conjugate form) so we can estimate the parameters and their upper and lower bounds.

```{r}
# Log-likelihood function of negative binomial with gamma parameterisation. Parameters have gamma priors.
lk.nbinom <- function(par, x, d) {
  if (min(par) < 0) {
    return(-Inf)
  }
  r <- par[1]
  p <- par[2]/(par[2] + d)
  sum(dnbinom(x, r, p, log=TRUE))
}

# Gibbs sampler. target is function accepting parameter vector. initial is starting vector, to be accepted by target.
gibbs <- function(target, initial, iterations=10000, warmup=2000, thin=100) {
  k <- length(initial)
  chain <- matrix(0, iterations + warmup, k)
  
  chain[1, ] <- initial
  
  for (i in 2:(iterations + warmup)) {
    current <- chain[i - 1, ]
    candidate <- chain[i - 1, ]
    for (j in 1:k) {
      candidate[j] <- rnorm(1, mean=current[j], sd=1)
      A <- exp(target(candidate) - target(current))
      if (runif(1) < A) {
        current <- candidate
      } else {
        candidate <- current
      }
    }
    chain[i, ] <- current
  }
  return(chain[seq(warmup, iterations + warmup, thin), ])
}

estimate_correlation <- function(x, d) {
  target <- function(par) lk.nbinom(par, x, d)
  parameters <- optim(c(1, 1), function(par) -target(par))$par
  
  a_ <- parameters[1]
  b_ <- parameters[2]
  
  S <- 1/sqrt(a_)
  mu <- a_/b_
  
  I <- mu * hmean(d)
  
  list(cor=(S * sqrt(I))/sqrt(1 + S^2 * I), S=S, I=I)
}
```

Define a simulation function to compute the true and estimated correlations and explained variances for a generic data generation function.

```{r}
# Simulate data collection and compute estimated and true correlation and explained variance. 
run_simulations <- function (iterations, generate_data) {
  # Create empty data frame to put results in.
  results <- data.frame(cors_true=numeric(), cors_est=numeric())
  
  for (i in 1:iterations) {
    # Generate data
    data <- generate_data()
    alpha <- data$alpha
    alpha_hat <- data$alpha_hat
    d <- data$d
    
    # Compute true correlation between actual and sampled rates.
    cor_true <- cor(alpha, alpha_hat)
    cor_est <- estimate_correlation(alpha_hat * d, d)$cor
    
    
    results[nrow(results) + 1, ] <- c(cor_true, cor_est)
  }
  
  results
}
```

Define data generation functions: 

* `generate_data_poisson()` - generates data according to the intended distribution, with interaction rates distributed by a gamma distribution, and number of interactions given by a Poisson.
* `generate_data_sbm()` - generates data with a community structure, where the interaction rates for dyads within the same group are drawn from a gamma with parameters $\alpha_W, \beta_W$ and dyads between groups from a gamma with parameters $\alpha_B, \beta_B$ where $\alpha_B < \alpha_W$ and $\beta_B < \beta_W$.
* `generate_data_norm()` - generates data breaking assumptions of the method, where interaction rates are distributed according to a beta, and number of interactions is drawn from a half-normal distribution.

```{r}
# Generate data from Poisson distribution with gamma distributed associations.
generate_data_poisson <- function() {
  m <- sample(c(10, 20, 50, 100), size=1)^2 # Number of dyads
  
  # Random mean sampling effort.
  mean_sampling <- runif(1, 1, 10)
  
  cv <- runif(1, 0, 2)
  mu <- runif(1, 0, 10)
  
  pi <- runif(1, 0, 1)
  
  a <- 1/cv**2
  b = a/mu
  
  # Variable sampling effort.
  d <- rpois(m, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rgamma(m, a, b) # Actual rate.
  alpha_hat <- rpois(m, alpha*d)/d # Sampled rate.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling, a=a, b=b)
}

generate_data_poisson_zi <- function() {
  m <- sample(c(10, 20, 50, 100), size=1)^2 # Number of dyads
  
  # Random mean sampling effort.
  mean_sampling <- runif(1, 1, 10)
  
  # Generate population parameters for gamma using mean interaction rate and desired network correlation.
  # mu <- runif(1, 1, 10)
  # rho <- runif(1, 0, 1.0)
  # b <- mean_sampling * (1 - rho^2)/rho^2
  # a <- b * mu
  
  cv <- runif(1, 0, 2)
  mu <- runif(1, 0, 1)
  
  pi <- runif(1, 0, 1)
  
  a <- 1/cv**2
  b = a/mu
  
  # Variable sampling effort.
  d <- rpois(m, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rgamma(m, a, b) # Actual rate.
  alpha[runif(m) < (1 - pi)] <- 0
  alpha_hat <- rpois(m, alpha*d)/d # Sampled rate.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling, a=a, b=b)
}

# Generate data from a weighted stochastic block model.
generate_data_sbm <- function() {
  m <- sample(c(10, 20, 50, 100), size=1)^2 # Number of dyads
  
  # Random mean sampling effort.
  mean_sampling <- runif(1, 10, 1000)
  
  # Generate population parameters for gamma using mean interaction rate and desired network correlation.

  cv <- runif(1, 0, 0.01)
  mu_w <- runif(1, 1, 10)
  mu_b <- runif(1, 1, mu_w)
  
  a <- 1/cv**2
  b_w = a/mu_w
  b_b = a/mu_b
  
  # b <- mean_sampling * (1 - rho^2)/rho^2
  # a_w <- b * mu_w
  # a_b <- b * mu_b
  
  # Variable sampling effort.
  d <- rpois(m, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  p <- runif(1, 0, 1) # Proportion of dyads in the same group.

  same_group <- runif(m) < p # TRUE/FALSE indicating dyads between same group (or NOT).

  alpha <- same_group * rgamma(m, a, b_w) + (1 - same_group) * rgamma(m, a, b_b)
  alpha_hat <- rpois(m, alpha*d)/d
  
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling, a=NA, b=b)
}

# Generate data using a beta interaction rate, and Poisson count.
generate_data_beta <- function() {
  m <- sample(c(5, 10, 20, 50, 100), size=1)^2 # Number of dyads
  
  # Random mean sampling effort.
  mean_sampling <- runif(1, 1, 100)
  
  # Generate population parameters for gamma using mean interaction rate and desired network correlation.
  mu <- runif(1, 0, 1)
  cv <- runif(1, 0, min(1.00, sqrt(1/mu - 1)))
  
  a <- (1 - mu - cv^2*mu)/cv^2
  b <- (cv^2 * mu * (mu - 1) + mu * (mu - 2) + 1)/(cv^2 * mu)
  
  # Variable sampling effort.
  d <- rpois(m, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rbeta(m, a, b) # Actual rate.
  # alpha <- runif(m, min=0, max=10)
  alpha_hat <- rpois(m, alpha*d)/d # Sampled rate.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling, a=a, b=b)
}

# Generate data using a beta interaction rate, and binomial count.
generate_data_betabin <- function() {
  m <- sample(c(5, 10, 20, 50, 100), size=1)^2 # Number of dyads
  
  # Random mean sampling effort.
  mean_sampling <- runif(1, 1, 100)
  
  # Generate population parameters for gamma using mean interaction rate and desired network correlation.
  mu <- runif(1, 0, 1)
  cv <- runif(1, 0, min(1.00, sqrt(1/mu - 1)))
  
  a <- (1 - mu - cv^2*mu)/cv^2
  b <- (cv^2 * mu * (mu - 1) + mu * (mu - 2) + 1)/(cv^2 * mu)
  
  # Variable sampling effort.
  d <- rpois(m, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rbeta(m, a, b) # Actual rate.
  # alpha <- runif(m, min=0, max=10)
  alpha_hat <- rbinom(m, d, alpha)/d # Sampled rate.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling, a=a, b=b)
}

generate_data_poisson_biased <- function() {
  m <- sample(c(10, 20, 50, 100), size=1)^2 # Number of dyads
  
  cv <- runif(1, 0, 2)
  mu <- runif(1, 0, 10)
  
  pi <- runif(1, 0, 1)
  
  a <- 1/cv**2
  b = a/mu
  
  # Variable sampling effort.
  k <- round(m/2)
  d <- c(rpois(k, runif(1, 1, 10)), rpois(m - k, runif(1, 10, 100)))
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rgamma(m, a, b) # Actual rate.
  alpha_hat <- rpois(m, alpha*d)/d # Sampled rate.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=NA, a=a, b=b)
}
```

## Poisson data

```{r}
results.pois <- run_simulations(200, generate_data_poisson)
write.csv(results.pois, "results/poisson.csv")
```


```{r}
results <- read.csv("results/poisson.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point(color=cp[3], alpha=0.5) +
  geom_abline() +
  geom_smooth(color=cp[8], size=1, se=FALSE) +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation") +
  theme_classic()

ggsave("figures/correlation_comparison_poisson.png", width=3, height=3)
```

```{r}
cat("Mean: \n")
mean(results$cors_true - results$cors_est)
cat("Standard deviation: \n")
sd(results$cors_true - results$cors_est)

cat("Correlation: \n")
cor(results$cors_true, results$cors_est)

cat("Mean absolute error: \n")
mean(abs(results$cors_true - results$cors_est))
```

## Community structured data

```{r}
results.sbm <- run_simulations(200, generate_data_sbm, plot_chains=FALSE)
write.csv(results.sbm, "results/sbm.csv")
```

```{r}
results <- read.csv("results/sbm.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point(color=cp[3], alpha=0.5) +
  geom_abline() +
  geom_smooth(color=cp[8], size=1, se=FALSE) +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation") +
  theme_classic()

ggsave("figures/correlation_comparison_sbm.png", width=3, height=3)
```

```{r}
cat("Mean: \n")
mean(results$cors_true - results$cors_est)
cat("Standard deviation: \n")
sd(results$cors_true - results$cors_est)

cat("Correlation: \n")
cor(results$cors_true, results$cors_est)

cat("Mean absolute error: \n")
mean(abs(results$cors_true - results$cors_est))
```

## Beta data

```{r}
results.beta <- run_simulations(200, generate_data_beta, plot_chains=FALSE)
write.csv(results.beta, "results/beta.csv")
```

```{r}
results <- read.csv("results/beta.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point(color=cp[3], alpha=0.5) +
  geom_abline() +
  geom_smooth(color=cp[8], size=1, se=FALSE) +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation") +
  theme_classic()

ggsave("figures/correlation_comparison_beta.png", width=6, height=4)
```

```{r}
cat("Mean: \n")
mean(results$cors_true - results$cors_est)
cat("Standard deviation: \n")
sd(results$cors_true - results$cors_est)

cat("Correlation: \n")
cor(results$cors_true, results$cors_est)

cat("Mean absolute error: \n")
mean(abs(results$cors_true - results$cors_est))
```

## Zero-inflated Poisson data

```{r}
results.pois.zi <- run_simulations(200, generate_data_poisson_zi)
write.csv(results.pois.zi, "results/poisson_zi.csv")
```

```{r}
results <- read.csv("results/poisson_zi.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point(color=cp[3], alpha=0.5) +
  geom_abline() +
  geom_smooth(color=cp[8], size=1, se=FALSE) +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation") +
  theme_classic()

ggsave("figures/correlation_comparison_poisson_zi.png", width=3, height=3)
```

```{r}
cat("Mean: \n")
mean(results$cors_true - results$cors_est)
cat("Standard deviation: \n")
sd(results$cors_true - results$cors_est)

cat("Correlation: \n")
cor(results$cors_true, results$cors_est)

cat("Mean absolute error: \n")
mean(abs(results$cors_true - results$cors_est))
```

## Beta binomial data

```{r}
results.norm <- run_simulations(200, generate_data_betabin)
write.csv(results.norm, "results/betabin.csv")
```

```{r}
results <- read.csv("results/betabin.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point(color=cp[3], alpha=0.5) +
  geom_abline() +
  geom_smooth(color=cp[8], size=1, se=FALSE) +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation") +
  theme_classic()

ggsave("figures/correlation_comparison_betabin.png", width=6, height=4)
```

```{r}
cat("Mean: \n")
mean(results$cors_true - results$cors_est)
cat("Standard deviation: \n")
sd(results$cors_true - results$cors_est)

cat("Correlation: \n")
cor(results$cors_true, results$cors_est)

cat("Mean absolute error: \n")
mean(abs(results$cors_true - results$cors_est))
```

```{r}
results.pois <- run_simulations(200, generate_data_poisson_biased)
write.csv(results.pois, "results/poisson_biased.csv")
```


```{r}
results <- read.csv("results/poisson_biased.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point(color=cp[3], alpha=0.5) +
  geom_abline() +
  geom_smooth(color=cp[8], size=1, se=FALSE) +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation") +
  theme_classic()

ggsave("figures/correlation_comparison_poisson_biased.png", width=6, height=4)
```

```{r}
cat("Mean: \n")
mean(results$cors_true - results$cors_est)
cat("Standard deviation: \n")
sd(results$cors_true - results$cors_est)

cat("Correlation: \n")
cor(results$cors_true, results$cors_est)

cat("Mean absolute error: \n")
mean(abs(results$cors_true - results$cors_est))
```

```{r}
?latent_space


```