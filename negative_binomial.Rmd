---
title: "Estimating Accuracy of Sampling with Interaction Rate Data"
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---

## Derivation

# Estimating Accuracy of Sampling with Interaction Rate Data - Derivation

If we assume the population association rates $\lambda_i$ are gamma-distributed, we have:
$$
\lambda_i \sim \text{Gamma}(\alpha, \beta).
$$
where $\alpha, \beta$ are the population-level parameters.

The observed number of interactions $X_i$ for the $i$-th dyad is likely to be Poisson-distributed, and will depend on the number of times the dyad were observed $d_i$. This gives us:
$$
X_i \sim \text{Poisson}(d_i \lambda_i) .
$$
Since $d_i$ is a constant, the distribution of $d_i \lambda_i$ is given by
$$
d_i \lambda_i \sim \text{Gamma}\left(\alpha, \frac{\beta}{d_i}\right).
$$
This kind of model is known as a Poisson-Gamma mixture, and can be described by the negative binomial in terms of the gamma parameters $\alpha, \beta$, and the sampling effort $d_i$:

$$
X_i \sim \text{NegBinomial}\left(\alpha, \frac{\beta}{\beta + d_i}\right).
$$

Typically when we estimate the association rate, we compute 
$$
\hat{\lambda_i} = \frac{X_i}{d_i}.
$$

Our aim is to compute the total variance in the estimated association rates $\hat{\lambda}_i$ explained by the total variance in the true association rates $\lambda_i$, that is:
$$
\text{EV} = \frac{\sum_{i=1}^n \text{Var}(\hat{\lambda}_i)}{\sum_{i=1}^n \text{Var}(\lambda_i)} = \frac{\hat{T}}{T}
$$
First, the variance of the true association rate is simply given by the variance of the gamma distribution:
$$
\text{Var}(\lambda_i) = \frac{\alpha}{\beta^2}.
$$

Summing over $n$ dyads we get
$$
T = \sum_{i=1}^n \text{Var}(\lambda_i) = \frac{n \alpha}{\beta^2}.
$$

The variance of the estimated association rates is slightly trickier, we use the variance of the negative binomial to get:
$$
\begin{aligned}
\text{Var}(\hat{\lambda_i})  &= \text{Var} \left( \frac{X_i}{d_i}\right) \\ &= \frac{1}{d_i^2} \text{Var}(X_i) \\ &= \frac{\alpha d_i (\beta + d_i)}{d_i^2\beta^2}.
\end{aligned}
$$

Again, summing over the $n$ dyads we get
$$
\begin{aligned}
\hat{T} &= \sum_{i=1}^n \text{Var}(\hat{\lambda}_i) \\ &=\sum_{i=1}^n \frac{\alpha d_i (\beta + d_i)}{d_i^2 \beta^2} \\ &= \frac{\alpha}{\beta^2} \sum_{i=1}^n \left(\frac{d_i^2 + d_i \beta}{d_i^2} \right) \\ &= \frac{\alpha}{\beta^2} \sum_{i=1}^n \left(1 + \frac{\beta}{d_i}\right) \\ &= \frac{\alpha}{\beta^2} \left(n + \beta \sum_{i=1}^n d_i^{-1}\right).
\end{aligned}
$$

Bringing these together, we get:
$$
\begin{aligned}
\text{EV} & = T\hat{T}^{-1} \\&= \frac{n\alpha}{\beta^2} \frac{\beta^2}{\alpha} \left(n + \beta \sum_{i=1}^n d_i^{-1}\right)^{-1} \\&= \frac{n}{n + \beta \sum_{i=1}^n d_i^{-1}}.
\end{aligned}
$$

The coefficient of variation/social differentiation of the interaction rate under this method is:
$$
S_\lambda = \text{CV}(\lambda) = \frac{\sigma(\lambda)}{\mu(\lambda)} = \frac{\alpha}{\beta^2} \frac{\beta}{\alpha} = \frac{1}{\beta}.
$$

So as $S_\lambda \rightarrow \infty$ we have $\text{EV} \rightarrow 1$, and as $S_\lambda \rightarrow 0$ we have $\text{EV} \rightarrow 0$. Therefore increases in social differentiation increase the explained variance. 

## Simulations

Load in ggplot for visualisations.

```{r setup}
knitr::opts_chunk$set(cache = TRUE)
library(ggplot2)
```

Define log-likelihood function of negative binomial with gamma parameters (in conjugate form) so we can estimate the parameters and their upper and lower bounds.

```{r}
# Log-likelihood function of negative binomial with gamma parameterisation. Parameters have gamma priors.
lk.nbinom <- function(par, x, d, priors=list(a=c(1, 0.5), b=c(1, 0.5))) {
  if (min(par) < 0) {
    return(-Inf)
  }
  r <- par[1]
  p <- par[2]/(par[2] + d)
  sum(dnbinom(x, r, p, log=TRUE)) + dgamma(par[1], priors$a[1], priors$a[2], log=TRUE) + dgamma(par[2], priors$b[1], priors$b[2], log=TRUE)
}

# Gibbs sampler. target is function accepting parameter vector. initial is starting vector, to be accepted by target.
gibbs <- function(target, initial, iterations=10000, warmup=2000, thin=100) {
  k <- length(initial)
  chain <- matrix(0, iterations + warmup, k)
  
  chain[1, ] <- initial
  
  for (i in 2:(iterations + warmup)) {
    current <- chain[i - 1, ]
    candidate <- chain[i - 1, ]
    for (j in 1:k) {
      candidate[j] <- rnorm(1, mean=current[j], sd=1)
      A <- exp(target(candidate) - target(current))
      if (runif(1) < A) {
        current <- candidate
      } else {
        candidate <- current
      }
    }
    chain[i, ] <- current
  }
  return(chain[seq(warmup, iterations + warmup, thin), ])
}
```

Define a simulation function to compute the true and estimated correlations and explained variances for a generic data generation function.

```{r}
# Simulate data collection and compute estimated and true correlation and explained variance. 
run_simulations <- function (iterations, generate_data, plot_chains=FALSE) {
  # Create empty data frame to put results in.
  results <- data.frame(
    mean_sampling=numeric(), 
    cors_true=numeric(), 
    cors_est=numeric(), 
    cors_est_lb=numeric(), 
    cors_est_ub=numeric(), 
    evs_true=numeric(),
    evs_est=numeric(),
    evs_est_lb=numeric(),
    evs_est_ub=numeric()
  )
  
  for (i in 1:iterations) {
    print(i)
    
    # Generate data
    data <- generate_data()
    
    alpha <- data$alpha
    alpha_hat <- data$alpha_hat
    d <- data$d
    mean_sampling <- data$mean_sampling
    
    n <- length(alpha)
    
    # Compute true correlation between actual and sampled rates.
    ev_true <- var(alpha)/var(alpha_hat)
    cor_true <- cor(alpha, alpha_hat)
    
    # Gibbs sample the log-likelihood of the negative binomial.
    target <- function(par) lk.nbinom(par, as.integer(alpha_hat * d), d)
    chain <- gibbs(target, c(1, 1), warmup=2000, iterations=10000, thin=10)
    
    if (plot_chains) {
      # Plot the chains for visual inspection.
      plot(chain[, 1], type="l", col=1, ylim=c(0, max(chain)))
      lines(chain[, 2], type="l", col=2)
    }  
    
    # Compute quantiles of b parameter for explained variance calculation.
    qt <- quantile(chain[, 2], probs=c(0.025, 0.5, 0.975))
    b_ <- qt[2]
    b_lb <- qt[1]
    b_ub <- qt[3]
    
    # Compute estimated explained variance and estimated correlation.
    ev_est <- n/(n + b_ * sum(d^-1))
    ev_est_lb <- n/(n + b_lb * sum(d^-1))
    ev_est_ub <- n/(n + b_ub * sum(d^-1))
    
    cor_est <- sqrt(ev_est)
    cor_est_lb <- sqrt(ev_est_lb)
    cor_est_ub <- sqrt(ev_est_ub)
    
    results[nrow(results) + 1, ] <- c(mean_sampling, cor_true, cor_est, cor_est_lb, cor_est_ub, ev_true, ev_est, ev_est_lb, ev_est_ub)
  }
  
  results
}
```

Define data generation functions: 

* `generate_data_poisson()` - generates data according to the intended distribution, with interaction rates distributed by a gamma distribution, and number of interactions given by a Poisson.
* `generate_data_sbm()` - generates data with a community structure, where the interaction rates for dyads within the same group are drawn from a gamma with parameters $\alpha_W, \beta_W$ and dyads between groups from a gamma with parameters $\alpha_B, \beta_B$ where $\alpha_B < \alpha_W$ and $\beta_B < \beta_W$.
* `generate_data_norm()` - generates data breaking assumptions of the method, where interaction rates are distributed according to a beta, and number of interactions is drawn from a half-normal distribution.

```{r}
# Generate data from Poisson distribution with gamma distributed associations.
generate_data_poisson <- function() {
  n <- 100 # Number of dyads
  # Generate population parameters for gamma.
  a <- runif(1, 1, 10)
  b <- runif(1, 1, 10)
  
  # Variable sampling effort.
  mean_sampling <- runif(1, 1, 20) # Random mean sampling effort.
  d <- rpois(n, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rgamma(n, a, b) # Actual rate.
  alpha_hat <- rpois(n, alpha*d)/d # Sampled rate.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling)
}

# Generate data from a weighted stochastic block model.
generate_data_sbm <- function() {
  a_w <- runif(1, 5, 20)
  b_w <- runif(1, 5, 20)
  a_b <- runif(1, 1, a_w)
  b_b <- runif(1, 1, b_w)
  
  n <- 100 # Number of dyads
  m <- 5 # Number of groups
  
  mean_sampling <- runif(1, 1, 20) # Random mean sampling effort.
  d <- rpois(n, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  k <- n/m # Number of dyads per group
  same_group <- runif(n) < m * k/n * (k - 1)/(n - 1) # TRUE/FALSE indicating dyads between same group (or NOT).

  alpha <- same_group * rgamma(n, a_w, b_w) + (1 - same_group) * rgamma(n, a_b, b_b)
  alpha_hat <- rpois(n, alpha*d)/d
  
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling)
}

# Generate data using a normal.
generate_data_norm <- function() {
  n <- 100 # Number of dyads
  # Generate population parameters for gamma.
  a <- runif(1, 1, 10)
  b <- runif(1, 1, 10)
  
  # Variable sampling effort.
  mean_sampling <- runif(1, 10, 20) # Random mean sampling effort.
  d <- rpois(n, mean_sampling) # Individual sampling efforts.
  d[d == 0] = 1 # Avoids division by zero error.
  
  # Generate association rates.
  alpha <- rbeta(n, a, b) # Actual rate.
  alpha_hat <- abs(rnorm(n, mean=alpha*d))/d # Approximation of half-normal, doesn't really matter as long as it isn't Poisson.
  list(alpha=alpha, alpha_hat=alpha_hat, d=d, mean_sampling=mean_sampling)
}
```

## Poisson data

```{r}
results.pois <- run_simulations(100, generate_data_poisson, plot_chains=FALSE)
write.csv(results.pois, "results/poisson.csv")
```

```{r}
results <- read.csv("results/poisson.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point() +
  geom_errorbar(aes(ymin=cors_est_lb, ymax=cors_est_ub)) +
  geom_smooth() +
  geom_abline() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation")

ggplot(results, aes(x=evs_true, y=evs_est)) +
  geom_point() +
  geom_errorbar(aes(ymin=evs_est_lb, ymax=evs_est_ub)) +
  geom_smooth() +
  geom_abline() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True explained variance") +
  ylab("Estimated explained variance")

ggplot(results, aes(x=mean_sampling, y=abs(cors_true - cors_est))) +
  geom_point() +
  geom_smooth()
```

## Community structured data

```{r}
results.sbm <- run_simulations(100, generate_data_sbm, plot_chains=FALSE)
write.csv(results.sbm, "results/sbm.csv")
```

```{r}
results <- read.csv("results/sbm.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point() +
  geom_errorbar(aes(ymin=cors_est_lb, ymax=cors_est_ub)) +
  geom_smooth() +
  geom_abline() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation")

ggplot(results, aes(x=evs_true, y=evs_est)) +
  geom_point() +
  geom_errorbar(aes(ymin=evs_est_lb, ymax=evs_est_ub)) +
  geom_smooth() +
  geom_abline() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True explained variance") +
  ylab("Estimated explained variance")

ggplot(results, aes(x=mean_sampling, y=abs(cors_true - cors_est))) +
  geom_point() +
  geom_smooth()
```

## Normal data

```{r}
results.norm <- run_simulations(100, generate_data_norm, plot_chains=FALSE)
write.csv(results.norm, "results/norm.csv")
```

```{r}
results <- read.csv("results/norm.csv")

ggplot(results, aes(x=cors_true, y=cors_est)) +
  geom_point() +
  geom_errorbar(aes(ymin=cors_est_lb, ymax=cors_est_ub)) +
  geom_smooth() +
  geom_abline() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True correlation") +
  ylab("Estimated correlation")

ggplot(results, aes(x=evs_true, y=evs_est)) +
  geom_point() +
  geom_errorbar(aes(ymin=evs_est_lb, ymax=evs_est_ub)) +
  geom_smooth() +
  geom_abline() +
  xlim(0, 1) +
  ylim(0, 1) +
  xlab("True explained variance") +
  ylab("Estimated explained variance")

ggplot(results, aes(x=mean_sampling, y=abs(cors_true - cors_est))) +
  geom_point() +
  geom_smooth()
```