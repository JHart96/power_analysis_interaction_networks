---
title: "R Notebook"
output: html_notebook
---

```{r}
library(igraph)
library(ggplot2)

cp <- c("#617899", "#38acaf", "#6fda97", "#c7e9b4", "#ffffcc")
cp <- c("#003f5c", "#2f4b7c", "#665191", "#a05195", "#d45087", "#f95d6a", "#ff7c43", "#ffa600")







node_regression <- function(x, y) {
  n <- length(y)
  X <- cbind(rep(1, n), x)
  obs <- .lm.fit(X, y)$coefficients
  null <- sapply(1:1000, function(i) .lm.fit(X, sample(y))$coefficients[[2]])
  pval <- mean(abs(obs[[2]]) <= abs(null))
  list(estimates=obs, pval=pval)
}

loss <- function(r, n, power) {
  if (r <= 0 || r >= 1) {
    return(Inf)
  }
  pvals <- rep(0, 200)
  for (i in 1:200) {
    x <- rnorm(n)
    
    lm_sd <- 1
    
    effect_size <- r * lm_sd * sqrt(1/(1 - r))/(sd(x) * sqrt(r + 1))
    y <- 1 + effect_size * x + rnorm(n, sd=lm_sd)
    pvals[i] <- summary(lm(y ~ x))$coefficients[2, 4]
  }
  (mean(pvals < 0.05) - power)^2
}

grid_search <- function(n, power) {
  search_range <- seq(0.0, 1.0, 0.05)
  
  losses <- c()
  for (i in 2:(length(search_range) - 1)) {
    # print(search_range[i])
    r <- search_range[i]
    
    losses <- c(losses, loss(r, n, power))
  }
  
  # search_range[which.min(losses) + 1]
  
  min_loss_idx <- which.min(losses) + 1
  min_loss <- search_range[min_loss_idx]
  
  search_range <- seq(min_loss - 0.05, min_loss + 0.05, 0.005)
  
  losses <- c()
  for (i in 2:(length(search_range) - 1)) {
    # print(search_range[i])
    r <- search_range[i]
    
    losses <- c(losses, loss(r, n, power))
  }
  
  r_ <- search_range[which.min(losses) + 1]
  r_
}
```

## Compute the R values required to achieve a given level of power.

```{r}
# # Find Pearson's r value for given number of nodes and statistical power.

pearson_rs <- data.frame(n=numeric(), r=numeric(), power=numeric(), error=numeric())
power <- 1.0

for (n in c(10, 20, 50, 100, 200)) {
  r <- grid_search(n, power)
  error <- loss(r, n, power)
  pearson_rs[nrow(pearson_rs) + 1, ] <- c(n, r, power, error)
}

pearson_rs
```

## Run main simulations using previously computed R values.

```{r}

results <- data.frame(rho=numeric(), correlation=numeric(), pval=numeric(), samples=numeric(), num_nodes=numeric(), pearson_r=numeric())

pb <- txtProgressBar(min=0, max=1000, style=3)

num_repeats <- 100

for (net_id in 1:1000) {
  # param_idx <- sample(1:length(n_r_params$n), size=1)
  param_idx <- sample(1:nrow(pearson_rs), size=1)
  
  n <- pearson_rs[param_idx, ]$n
  r <- pearson_rs[param_idx, ]$r

  s <- sample(seq(100, 10000, 100), size=1)

  mu <- runif(1, 1, 100)
  
  rho <- runif(1, 0, 1.0)
  b <- s * (1 - rho^2)/rho^2
  a <- b * mu
  
  lm_sd <- 1
  
  # Generate association rates.
  alpha <- rgamma(n^2, a, b) # Actual rate.

  net <- graph_from_adjacency_matrix(matrix(alpha, n, n), weighted=TRUE)

  metric <- strength(net)

  for (i in 1:num_repeats) {
    effect_size <- r * lm_sd * sqrt(1/(1 - r))/(sd(metric) * sqrt(r + 1))
    
    trait <- 1 + effect_size * metric + rnorm(n, sd=lm_sd)

    # Sampling
    d <- rpois(n^2, s)
    d[d == 0] <- 1
    
    alpha_hat <- rpois(n^2, alpha*d)/d # Sampled rate.
    
    # Build network
    net_est <- graph_from_adjacency_matrix(matrix(alpha_hat, n, n), weighted=TRUE)
    
    metric_est <- strength(net_est)
    # fit_summary <- summary(lm(trait ~ metric_est))
    
    fit_summary <- node_regression(metric_est, trait)
    # qplot(trait, metric_est)
    
    pval <- fit_summary$pval
    
    # if (dim(fit_summary$coefficients)[1] == 2) {
    #   pval <- summary(lm(trait ~ metric_est))$coefficients[2, 4]
    # } else {
    #   pval = NA
    # }
    
    correlation <- cor(alpha, alpha_hat)
    
    results[nrow(results) + 1, ] <- c(rho, correlation, pval, s, n, r)
  }
  setTxtProgressBar(pb, net_id)
}

close(pb)

results <- na.omit(results)

write.csv(results, "results/node_regression_power_100.csv")
```

## Plot results of main simulations.

```{r}
results <- read.csv("results/node_regression_power_100.csv")
results <- subset(results, num_nodes != 200) # Pearson r calculation for n=200 failed, so exclude it.

results_agg <- aggregate(pval ~ rho + samples + num_nodes + pearson_r, results, function(x) mean(x < 0.05))

ggplot(results_agg, aes(x=rho, y=pval, color=as.factor(num_nodes))) +
  geom_point(size=1) + 
  xlab("Correlation") +
  ylab("Power") +
  xlim(0, 1) +
  ylim(0, 1) +
  geom_smooth() +
  scale_color_manual(values=c(cp[4], cp[5], cp[6], cp[8])) +
  # scale_color_brewer(palette="Spectral") +
  theme_classic() +
  theme(legend.position = c(0.9, 0.25)) +
  labs(color="# nodes")

ggsave("figures/power.png", width=7, height=5)
```

### 

```{r}
# Do this for each n (or r).
power_levels <- seq(0.1, 0.9, 0.1)
power_table <- matrix(0, nrow=length(pearson_rs$n), ncol=length(power_levels))
rownames(power_table) <- pearson_rs$n
colnames(power_table) <- power_levels

for (i in 1:length(pearson_rs$n)) {
  for (j in 1:length(power_levels)) {
    n <- pearson_rs$n[i]
    power <- power_levels[j]
    lo <- loess(rho ~ pval, subset(results_agg, num_nodes==n))
    rho_ <- predict(lo, power)
    
    power_table[i, j] <- round(rho_, 4)
    # cat(paste("Number of nodes:", n, " Rho required: ", rho_, "\n"))
  }
}

power_table
write.csv(power_table, "results/power_table.csv")
```

## Sampling requirements

```{r}
S <- c(0.05, 0.2, 0.5, 0.8, 1.0)
N <- c("10", "20", "50", "100")

sampling_requirements <- matrix(0, nrow=5, ncol=4)
rownames(sampling_requirements) <- S
colnames(sampling_requirements) <- c("10", "20", "50", "100")

for (i in 1:length(N)) {
  rho_ <- power_table[N[i], "0.8"]
  I <- rho_^2/(S^2 * (1 - rho_^2))
  sampling_requirements[, i] <- ceiling(I)
}

sampling_requirements
write.csv(sampling_requirements, "results/sampling_requirements.csv")
```

## Power for given correlation at different network sizes.

```{r}
lo <- loess(pval ~ rho, subset(results_agg, num_nodes==50))
predict(lo, 0.5)

lo <- loess(pval ~ rho, subset(results_agg, num_nodes==100))
predict(lo, 0.5)
```


## 

```{r}
S <- c(0.05, 0.2, 0.5, 0.8, 1.0)
N <- c("10", "20", "50", "100")

sampling_requirements_node <- matrix(0, nrow=5, ncol=4)
rownames(sampling_requirements_node) <- S
colnames(sampling_requirements_node) <- c("10", "20", "50", "100")

for (i in 1:length(N)) {
  rho_ <- power_table[N[i], "0.8"]
  I <- rho_^2/(S^2 * (1 - rho_^2))
  sampling_requirements_node[, i] <- ceiling(2 * I/as.integer(N[i]))
}

sampling_requirements_node
write.csv(sampling_requirements_node, "results/sampling_requirements_node.csv")
```

## Run simulations to show 

```{r}
results <- data.frame(correlation=numeric(), pval=numeric(), mu=numeric(), cv=numeric(), samples=numeric(), num_nodes=numeric(), pearson_r=numeric())

pb <- txtProgressBar(min=0, max=2000, style=3)

num_repeats <- 20

for (net_id in 1:2000) {
  # param_idx <- sample(1:length(n_r_params$n), size=1)
  param_idx <- sample(1:nrow(pearson_rs), size=1)
  
  n <- pearson_rs[param_idx, ]$n
  r <- pearson_rs[param_idx, ]$r

  cv <- runif(1, 0, 0.5)
  mu <- runif(1, 0, 1)
  
  a <- 1/cv**2
  b = a/mu
  
  lm_sd <- 1
  
  # Generate association rates.
  alpha <- rgamma(n^2, a, b) # Actual rate.

  net <- graph_from_adjacency_matrix(matrix(alpha, n, n), weighted=TRUE)

  metric <- strength(net)
  
  s <- sample(seq(100, 2500, 100), size=1)

  for (i in 1:num_repeats) {
    effect_size <- r * lm_sd * sqrt(1/(1 - r))/(sd(metric) * sqrt(r + 1))
    
    trait <- 1 + effect_size * metric + rnorm(n, sd=lm_sd)

    # Sampling
    d <- rpois(n^2, s)
    d[d == 0] <- 1
    
    alpha_hat <- rpois(n^2, alpha*d)/d # Sampled rate.
    
    # Build network
    net_est <- graph_from_adjacency_matrix(matrix(alpha_hat, n, n), weighted=TRUE)
    
    metric_est <- strength(net_est)
    # fit_summary <- summary(lm(trait ~ metric_est))
    
    fit_summary <- node_regression(metric_est, trait)
    # qplot(trait, metric_est)
    
    pval <- fit_summary$pval
    
    # if (dim(fit_summary$coefficients)[1] == 2) {
    #   pval <- summary(lm(trait ~ metric_est))$coefficients[2, 4]
    # } else {
    #   pval = NA
    # }
    
    correlation <- cor(alpha, alpha_hat)
    
    results[nrow(results) + 1, ] <- c(correlation, pval, mu, cv, s, n, r)
  }
  
  results
  
  setTxtProgressBar(pb, net_id)
}

close(pb)

results <- na.omit(results)

write.csv(results, "results/node_regression_sample_size_100.csv")
```

```{r}
results <- read.csv("results/node_regression_sample_size_100.csv")
results_agg <- aggregate(pval ~ mu + cv + samples + num_nodes + pearson_r, results, function(x) mean(x < 0.05))
results_agg
```

```{r}
mean(results_agg$pval)
```

```{r}
ggplot(results_agg, aes(x=cv, y=samples, color=cut(pval, breaks=4))) +
  geom_point() +
  geom_line(aes(x=social_differentiations[2:2001], y=optimal_sampling[2:2001]), color="black", size=1) +
  xlim(0, 0.5) +
  scale_color_manual(values = c(cp[4], cp[5], cp[6], cp[8])) +
  theme(legend.position = c(0.8, 0.7))

ggsave("figures/elbow_samples.png", width=5, height=5)
```

