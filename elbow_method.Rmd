---
title: "Elbow method for computing optimal network correlation"
output: html_notebook
---

Load in results from node regression power analysis and fit a power predictor to it to predict power given correlation.

```{r}
results <- read.csv("results/node_regression_power_100.csv")
results <- subset(results, num_nodes != 200) # Pearson r calculation for n=200 failed, so exclude it.

results_agg <- aggregate(pval ~ rho + samples + num_nodes + pearson_r, results, function(x) mean(x < 0.05))
lo_power <- loess(results_agg$pval ~ results_agg$rho)
```

Define correlation function again.

```{r}
rho <- function(mu, S, D) (S * sqrt(mu * D))/(sqrt(1 + mu * S^2 * D))
```

Compute elbow point and find the correlation & number of samples it corresponds to.

```{r}
elbow_points <- rep(0, 1)
optimal_sampling <- rep(0, 1)

# social_differentiations <- c(0.2, 0.5, 0.8)
social_differentiations <- seq(1/2000, 1.0, 1/2000)

for (i in 1:length(social_differentiations)) {
  I <- seq(0, 10000, 1)
  mu <- 10
  D <- I/mu
  S <- social_differentiations[i]
  
  rho_ <- rho(mu, S, D)
  
  theta <- atan2(max(rho_) - min(rho_), max(I) - min(I))
  co = cos(theta)
  si = sin(theta)
  rotation_matrix = matrix(c(co, -si, si, co), nrow=2, ncol=2)
  
  rotated_data <- cbind(rho_, I) %*% rotation_matrix
  
  rho_prime <- rho_[which.max(rotated_data[, 1])] # Optimal rho, by diminishing returns principle.
  
  I_prime <- I[which.max(rotated_data[, 1])]
  
  elbow_points[i] <- rho_prime
  optimal_sampling[i] <- I_prime
}

power <-predict(lo_power, elbow_points)
power[power > 1] <- 1
```

Plot social differentiation against optimal correlation, corresponding optimal sampling, and power at optimal correlation.

```{r}
plot(social_differentiations, elbow_points, type="l") # CV vs correlation.
plot(social_differentiations, power, type="l") # CV vs power.
plot(social_differentiations, optimal_sampling, type="l") # CV vs optimal sampling.
```

Simulate node regression for different numbers of samples to make sure optimal sampling is using minimum number of required samples.

```{r}
results <- data.frame(correlation=numeric(), pval=numeric(), mu=numeric(), cv=numeric(), samples=numeric(), num_nodes=numeric(), pearson_r=numeric())

pb <- txtProgressBar(min=0, max=2000, style=3)

num_repeats <- 20

for (net_id in 1:2000) {
  # param_idx <- sample(1:length(n_r_params$n), size=1)
  param_idx <- sample(1:nrow(pearson_rs), size=1)
  
  n <- pearson_rs[param_idx, ]$n
  r <- pearson_rs[param_idx, ]$r

  cv <- runif(1, 0, 0.5)
  mu <- runif(1, 0, 1)
  
  a <- 1/cv**2
  b = a/mu
  
  lm_sd <- 1
  
  # Generate association rates.
  alpha <- rgamma(n^2, a, b) # Actual rate.

  net <- graph_from_adjacency_matrix(matrix(alpha, n, n), weighted=TRUE)

  metric <- strength(net)
  
  s <- sample(seq(100, 2500, 100), size=1)

  for (i in 1:num_repeats) {
    effect_size <- r * lm_sd * sqrt(1/(1 - r))/(sd(metric) * sqrt(r + 1))
    
    trait <- 1 + effect_size * metric + rnorm(n, sd=lm_sd)

    # Sampling
    d <- rpois(n^2, s)
    d[d == 0] <- 1
    
    alpha_hat <- rpois(n^2, alpha*d)/d # Sampled rate.
    
    # Build network
    net_est <- graph_from_adjacency_matrix(matrix(alpha_hat, n, n), weighted=TRUE)
    
    metric_est <- strength(net_est)
    # fit_summary <- summary(lm(trait ~ metric_est))
    
    fit_summary <- node_regression(metric_est, trait)
    # qplot(trait, metric_est)
    
    pval <- fit_summary$pval
    
    # if (dim(fit_summary$coefficients)[1] == 2) {
    #   pval <- summary(lm(trait ~ metric_est))$coefficients[2, 4]
    # } else {
    #   pval = NA
    # }
    
    correlation <- cor(alpha, alpha_hat)
    
    results[nrow(results) + 1, ] <- c(correlation, pval, mu, cv, s, n, r)
  }
  
  results
  
  setTxtProgressBar(pb, net_id)
}

close(pb)

results <- na.omit(results)

write.csv(results, "results/node_regression_sample_size_100.csv")
```

Plot number of samples against social differentiation with power level shown in colour. Overlay optimal sampling.

```{r}
results <- read.csv("results/node_regression_sample_size_100.csv")
results_agg <- aggregate(pval ~ mu + cv + samples + num_nodes + pearson_r, results, function(x) mean(x < 0.05))

ggplot(results_agg, aes(x=cv, y=samples, color=cut(pval, breaks=4))) +
  geom_point() +
  geom_line(aes(x=social_differentiations[2:2001], y=optimal_sampling[2:2001]), color="black", size=1) +
  xlim(0, 0.5) +
  scale_color_manual(values = c(cp[4], cp[5], cp[6], cp[8])) +
  theme(legend.position = c(0.8, 0.7))

# ggsave("figures/elbow_samples.png", width=5, height=5)
```

```{r}
# Do cut() on cv and calculate proportion of false positives from curve for each break. In other words, analyse performance of line as discriminator.
```
